# ðŸ§  **CRAG Pipeline â€” Corrective Retrieval-Augmented Generation**

### *A Smarter, More Reliable Alternative to Traditional RAG*

Retrieval-Augmented Generation (RAG) is widely used to enhance LLMs with external knowledge. However, **traditional RAG suffers from a major flaw**:

### âŒ **It blindly trusts retrieved documentsâ€”even when they are irrelevant.**

This leads to:

* Hallucinations
* Incorrect answers
* Over-reliance on possibly wrong retrieved chunks
* Poor performance on out-of-domain questions

To overcome this behavior, researchers introduced **CRAG â€” Corrective RAG**.

---

# ðŸš€ **What is CRAG?**

**CRAG (Corrective Retrieval-Augmented Generation)** is an improved retrieval pipeline designed to:

### âœ”ï¸ Validate retrieved documents

### âœ”ï¸ Detect incorrect retrieval

### âœ”ï¸ Trigger fallback actions

### âœ”ï¸ Combine multiple knowledge channels

### âœ”ï¸ Reduce hallucinations by forcing corrective behavior

In CRAG, retrieval is followed by a **relevance evaluation step**. Based on this score:

| Relevance Score      | Action                                       |
| -------------------- | -------------------------------------------- |
| **High (> 0.7)**     | Use the retrieved document                   |
| **Low (< 0.3)**      | Discard retrieval â†’ switch to web search     |
| **Medium (0.3â€“0.7)** | Hybrid mode â†’ combine retrieval + web search |

This makes CRAG far more robust and accurate.

---

# ðŸ†š **CRAG vs Traditional RAG**

## ðŸŸ¦ **Traditional RAG**

* Retrieves top-k chunks
* Feeds them blindly to the LLM
* Assumes retrieval is always correct

**Problems:**

* If retrieval is irrelevant â†’ LLM produces wrong answers
* Cannot handle out-of-scope queries
* Does not adapt dynamically
* High hallucination risk

---

## ðŸŸ© **CRAG â€” Corrective RAG**

CRAG introduces a **corrective decision layer**, making it:

### â­ **More Accurate**

Irrelevant chunks are filtered via an LLM evaluator.

### â­ **More Adaptive**

If documents are irrelevant â†’ it switches to **web search**.

### â­ **More Reliable**

Hybrid mode ensures a balanced mix of local + online knowledge.

### â­ **Less Hallucinatory**

Only high-confidence knowledge is allowed into final generation.

---

# ðŸ“Š **Pipeline Overview (CRAG Architecture)**

```
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚        User Query        â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚ Retrieve Top-k    â”‚
              â”‚ (FAISS Vector DB) â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚ LLM-based Relevance Evaluator  â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚           â”‚
     High Score â”‚           â”‚ Low Score
    (> 0.7)     â”‚           â”‚ (< 0.3)
                â†“           â†“
     Use PDF Docâ”‚     Perform Web Search
         â†“       â”‚           â†“
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚ Knowledge     â”‚   â”‚ Web Knowledge â”‚
     â”‚ Refinement    â”‚   â”‚ Refinement    â”‚
     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
            â”‚                    â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚   Response Generation    â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# ðŸ§© **Advantages of CRAG**

### ðŸŸ¢ **1. High Accuracy**

Irrelevant chunks are filtered out before reaching the LLM.

### ðŸŸ¢ **2. Scalable & Domain-Agnostic**

Works for:

* LLM apps
* PDF QA
* Web-assisted question answering
* Hybrid knowledge systems

### ðŸŸ¢ **3. Reduced Hallucinations**

CRAG only uses **validated** or **trusted** knowledge.

### ðŸŸ¢ **4. Fallback Mechanism**

If retrieval fails â†’ automatic web search.

### ðŸŸ¢ **5. Hybrid Reasoning**

CRAG combines:

* Vector DB knowledge
* Web knowledge
* LLM reasoning

â€¦based on confidence.

### ðŸŸ¢ **6. Better handling of out-of-domain queries**

Traditional RAG fails when query â‰  document domain.
CRAG performs a **dynamic correction**.

---

# ðŸ§  **CRAG Implementation**


âœ” **PDF processing** with LangChain
âœ” **Text splitting** (RecursiveCharacterSplitter)
âœ” **Embeddings** (HuggingFace MiniLM)
âœ” **FAISS Vectorstore**
âœ” **Groq Llama 3.3 70B LLM**
âœ” **Relevance scoring using structured output**
âœ” **Knowledge refinement**
âœ” **Query rewriting for web search**
âœ” **Fallback logic**
âœ” **Final answer generation with sources**

Below is a breakdown for each stage:

---

## ðŸ“¥ 1. **PDF Encoding**

* Load PDF using `PyPDFLoader`
* Split into chunks
* Remove weird tab characters
* Convert to embeddings using `all-MiniLM-L6-v2`
* Store vectors in FAISS

---

## ðŸ” 2. **Document Retrieval**

```
docs = faiss_index.similarity_search(query, k=3)
```

Retrieves top-k based on cosine similarity.

---

## ðŸ§ª 3. **Evaluation (CRAG Correction Layer)**

The evaluator uses:

```python
class RetrievalEvaluatorInput(BaseModel):
    relevance_score: float
```

LLM decides a score between **0 and 1**.

---

## ðŸ”„ 4. **Decision Logic**

### If **score > 0.7**

â†’ Use retrieved document

### If **score < 0.3**

â†’ Web search (DuckDuckGo)

### If **0.3â€“0.7**

â†’ Combine retrieval + web search

This is the **core of CRAG**.

---

## ðŸ“ 5. **Knowledge Refinement**

Extracts bullet-point knowledge from documents or search results.

---

## ðŸŒ 6. **Web Search + Query Rewriting**

If retrieval fails:

* Query â†’ rewritten for search
* DuckDuckGo returns results
* Key information extracted

---

## ðŸ§¾ 7. **Final Answer Generation**

Adds:

* context
* reasoning
* sources with links

---

# ðŸ§ª **Sample Output (From Your Run)**

Your pipeline correctly detected:

### Query 1: *"What are the main causes of climate change?"*

* Retrieved documents relevant (score 0.8)
  â†’ Uses PDF-based knowledge.

### Query 2: *"How did Harry beat Quirrell?"*

* Retrieval totally irrelevant (score 0.0)
  â†’ CRAG switched to **web search**.
  â†’ Extracted relevant story info.

This demonstrates CRAG working exactly as intended.

---

# ðŸ **Conclusion**

CRAG is a **superior evolution of RAG**, designed for reliability, correctness, and adaptive knowledge retrieval.

---